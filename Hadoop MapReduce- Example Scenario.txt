Hadoop - MapReduce
----------------------------------------

MapReduce is a framework using which we can write applications to process huge amounts of data, in parallel, on large clusters of commodity hardware in a reliable manner.


Example Scenario
--------------------------
Given below is the data regarding the electrical consumption of an organization. It contains the monthly electrical consumption and the annual average for various years.

If the below data is given as input, we have to write applications to process it and produce results such as finding the year of maximum usage.


Input Data (sample.txt)
-------------------------------------
1979   23   23   23   43   24   25   26   26   26   26   25   26  25 
1980   26   27   28   28   28   30   31   31   31   30   30   30  29 
1981   31   32   32   32   33   34   35   36   36   34   34   34  34 
1984   39   38   39   39   39   41   42   43   40   39   38   38  40 
1985   38   39   39   39   39   41   41   41   00   40   39   39  45

Example Program (ProcessUnits.java)
-----------------------------------------------------
package hadoop; 

import java.util.*; 

import java.io.IOException; 
import java.io.IOException; 

import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.conf.*; 
import org.apache.hadoop.io.*; 
import org.apache.hadoop.mapred.*; 
import org.apache.hadoop.util.*; 

public class ProcessUnits {
   //Mapper class 
   public static class E_EMapper extends MapReduceBase implements 
   Mapper<LongWritable ,/*Input key Type */ 
   Text,                /*Input value Type*/ 
   Text,                /*Output key Type*/ 
   IntWritable>        /*Output value Type*/ 
   {
      //Map function 
      public void map(LongWritable key, Text value, 
      OutputCollector<Text, IntWritable> output,   
      
      Reporter reporter) throws IOException { 
	String[] splits = value.toString().split("\\s+"); 
	String year = splits[0];
	String lasttoken = splits[splits.length - 1]; 

         int avgprice = Integer.parseInt(lasttoken); 
         output.collect(new Text(year), new IntWritable(avgprice)); 
      } 
   }
   
   //Reducer class 
   public static class E_EReduce extends MapReduceBase implements Reducer< Text, IntWritable, Text, IntWritable > {
   
      //Reduce function 
      public void reduce( Text key, Iterator <IntWritable> values, 
      OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { 
         int maxavg = 30; 
         int val = Integer.MIN_VALUE; 
            
         while (values.hasNext()) { 
            if((val = values.next().get())>maxavg) { 
               output.collect(key, new IntWritable(val)); 
            } 
         }
      } 
   }

   //Main function 
   public static void main(String args[])throws Exception { 
      JobConf conf = new JobConf(ProcessUnits.class); 
      
      conf.setJobName("max_eletricityunits"); 
      conf.setOutputKeyClass(Text.class);
      conf.setOutputValueClass(IntWritable.class); 
      conf.setMapperClass(E_EMapper.class); 
      conf.setCombinerClass(E_EReduce.class); 
      conf.setReducerClass(E_EReduce.class); 
      conf.setInputFormat(TextInputFormat.class); 
      conf.setOutputFormat(TextOutputFormat.class); 
      
      FileInputFormat.setInputPaths(conf, new Path(args[0])); 
      FileOutputFormat.setOutputPath(conf, new Path(args[1])); 
      
      JobClient.runJob(conf); 
   } 
}

Compilation and Execution of Process Units Program
-------------------------------------------------------------------------------


 Start the Hadoop System by using 

	start-all.sh
	

Step 1 - Create Input Data (sample.txt) using below text

	nano sample.txt   ( paste the below text)

		1979   23   23   23   43   24   25   26   26   26   26   25   26  25 
		1980   26   27   28   28   28   30   31   31   31   30   30   30  29 
		1981   31   32   32   32   33   34   35   36   36   34   34   34  34 
		1984   39   38   39   39   39   41   42   43   40   39   38   38  40 
		1985   38   39   39   39   39   41   41   41   00   40   39   39  45

	save & exit 	


Step 2 - Create the java program.

	nano ProcessUnits.java   

		//paste the program here

	save & exit 

Step  3-  Create a directory to store the compiled java classes.

	$  mkdir units

Step 4 - Download Hadoop-core-1.2.1.jar, which is used to compile and execute the MapReduce program


Step 5 - Compile the ProcessUnits.java program and create a jar for the program.
	
	$  javac -classpath hadoop-core-1.2.1.jar -d units ProcessUnits.java 
	$  jar -cvf units.jar -C units/  .

Step 6 - Create an input directory in HDFS.
	$  hadoop fs -mkdir /input_dir 
	
Step 7 - Copy the input file named sample.txt in the input directory of HDFS.
	
	$  hadoop fs -put sample.txt /input_dir 

Step 8 - Verify the files in the input directory.
	
	$  hadoop fs -ls /input_dir

Step 9 - Run the Application by taking the input files from the input directory.
	
	$  hadoop jar units.jar hadoop.ProcessUnits /input_dir /output_dir

Step 10 - Verify the resultant files in the output folder.
	
	$  hadoop fs -ls /output_dir

Step 11 - View the output by using Part-00000 file. 

	$  hadoop fs -cat  /output_dir/part-00000 

Sample Output
----------------------
hadoopuser@ubuntu:~$ hadoop fs -cat /output_dir/part-00000 
	1981	34
	1984	40
	1985	45
hadoopuser@ubuntu:~$ 


------------------------------------------------------------------------------------------
